{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= \"./resources/title.png\">\n",
    "\n",
    "### Contents:\n",
    "\n",
    "1. Why Linear Algebra?\n",
    "2. Data Types for Linear Algebra\n",
    "3. Operations on Vectors and Matrices\n",
    "    - Addition and Subtraction\n",
    "    - Broadcasting\n",
    "    - Multiplication (3 kinds)\n",
    "    - Division\n",
    "4. Systems of Linear Equations\n",
    "5. Linear Regression with Linear Algebra\n",
    "\n",
    "\n",
    "## notes\n",
    "\n",
    "- visualization of vectors\n",
    "- contextualize operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1. Why Linear Algebra?\n",
    "\n",
    "Linear Algebra is the basis of many machine learning models.\n",
    "\n",
    "Data is usually already set up into a matrix by default!\n",
    "\n",
    "<img src= \"./resources/dataset.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It can be used to model complicated things like language\n",
    "\n",
    "<img src = \"./resources/Word-Vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important for image compression and recognition\n",
    "\n",
    "<img src = \"./resources/images.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recommendation engines are able to make much more sophisticated recommendations by using linear algebra in conjunction with user and content data.\n",
    "\n",
    "<img src = \"./resources/netflix.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./resources/linalgmeme.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Types for Linear Algebra\n",
    "\n",
    "<img src = \"./resources/datadogs.jpg\">\n",
    "\n",
    "* Scalars only have magnitude.\n",
    "\n",
    "* A vector is an array with **magnitude and direction**. The coordinates of a vector represent where the tip of the vector would be if you travelled from the origin, and the magnitude of a vector would be its length in space.\n",
    "\n",
    "* Matrices can be interpreted differently in different contexts but it's often used to represent multiple simultaneous vectors. \n",
    "\n",
    "* A vector or matrix can be multiplied by a scalar to create a change in **scale** and/or direction.\n",
    "\n",
    "* Tensors are made up of matrices with the same dimensions.\n",
    "\n",
    "Vectors, matrices and tensors are represented by NumPy arrays. **Not lists!!!** We can use `np.array.shape` to explore the dimensions of these data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([1, 2, 3, 4, 5, 6])\n",
    "matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "matrix2 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "tensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "# tensor10 = np.array([[natrix1]])\n",
    "\n",
    "print(vector)\n",
    "print('vector shape:', vector.shape, '\\n')\n",
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')\n",
    "print(matrix2)\n",
    "print('matrix2 shape:', matrix2.shape, '\\n')\n",
    "print(tensor)\n",
    "print('tensor shape:', tensor.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mess around with indexing a tensor\n",
    "\n",
    "tensor[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of a vector\n",
    "\n",
    "$length = \\|v \\| = \\sqrt{v \\cdot v} = (v_{1}^2 + v_{2}^2 + \\cdot \\cdot \\cdot \\cdot + v_{n}^2)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing\n",
    "\n",
    "Calling `.transpose()` on an array **reverses** its shape order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')\n",
    "\n",
    "# transposed\n",
    "print(matrix1.transpose(), '\\n')\n",
    "print('matrix1.transpose() shape:', matrix1.transpose().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], \n",
    "           [[13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]]])\n",
    "\n",
    "print(tensor2, '\\n')\n",
    "print('shape of tensor2: ', tensor2.shape, '\\n')\n",
    "\n",
    "# transposed\n",
    "print(tensor2.transpose(), '\\n')\n",
    "print('shape of tensor2.transpose(): ', tensor2.transpose().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Operations on Vectors and Matrices\n",
    "\n",
    "## a. Addition & Subtraction\n",
    "\n",
    "For addition and subtraction of vectors, matrices and even tensors, as long as the dimensions are equal the operation occurs **element-wise**. This means given two vectors `v` and `w`:\n",
    "\n",
    "$ \\vec{v} = \\begin{bmatrix}v_{1} \\\\v_{2}\\end{bmatrix} $\n",
    "\n",
    "\n",
    "$ \\vec{w} = \\begin{bmatrix}w_{1} \\\\w_{2}\\end{bmatrix} $\n",
    "\n",
    "$ \\vec{v} + \\vec{w} = \\begin{bmatrix}v_{1} + w_{1} \\\\v_{2} + w_{2}\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical example \n",
    "\n",
    "v = np.array([2, 4])\n",
    "w = np.array([3, 2])\n",
    "v - w # what about v-w?\n",
    "# what does this look like graphically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Broadcasting\n",
    "\n",
    "NumPy arrays allow for something known as **broadcasting**, which happens when you perform operations across arrays with different number of dimensions. NumPy makes duplicates of the lower-dimension array **as long as the higher-dimension array *contains* the same shape** in order to execute the operation. Order of the `.shape` tuple matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scalar = 4\n",
    "print(vector)\n",
    "\n",
    "print(vector + scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 0, 1])\n",
    "m1 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "m1 - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shapes of matrices and vectors can be broadcast onto `tensor2`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes here: \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Multiplication\n",
    "\n",
    "### i. Hadamard Product\n",
    "\n",
    "Hadamard Multiplication occurs element-wise, and therefore can only occur between matrices of the same shape **OR** when broadcasting can occur. The Hadamard product is very easy in NumPy: just use `*`!\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\circ\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} & a_{1,2}\\times b_{1,2} \\\\\n",
    "a_{2,1}\\times b_{2,1} & a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NumPy to calculate the Hadamard product of\n",
    "# [[1, 2], [1, 2]] and [[3, 4], [5, 9]]\n",
    "\n",
    "# Your code here:\n",
    "a = np.array([[1,2], [1,2]])\n",
    "b = np.array([[3,4], [5,9]])\n",
    "a * b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Dot Product\n",
    "\n",
    "Dot product is probably the most relevant kind of multiplication for our use cases. The dot product of matrices is also commonly known as **Matrix Multiplication**. Unless otherwise stated, _multiplication_ refers to this kind of multiplication.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the **rows** (horizontal) of the first matrix and do an element-wise product with the **columns** (vertical) of the second matrix. With this rule, what are the shape restrictions to doing matrix multiplication?\n",
    "\n",
    "i.e. if we want to multiply matrices $A$ and $B$ where the dimensions of $A$ and $B$ are $m \\times n$ and $p \\times q$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes here!\n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Using NumPy arrays, dot-multiply the matrices\n",
    "\\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "5 & 7\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 4 \\\\\n",
    "3 & 10\n",
    "\\end{bmatrix}\n",
    "\n",
    "in the code-cell below using `np.dot()`. Look up the documentation! Remember that you need square brackets around the whole array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Your code here: \n",
    "\n",
    "a = np.array([[3,2],[5,7]])\n",
    "b = np.array([[2,4],[3,10]])\n",
    "np.dot(b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, $AB ≠ BA $  and $(AB)C ≠ A(BC)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Cross Product\n",
    "\n",
    "The cross product is used for more abstract applications of linear algebra and here we'll just define it for vectors.\n",
    "\n",
    "The result of the cross product of two vectors: $A \\times B = |A||B|sin( \\theta) n $\n",
    "- ($n$ is the unit vector perpendicular to the plane formed by the two vectors)\n",
    "- has a magnitude (length) equal to the area of the parallelogram formed by the two vectors\n",
    "- perpendicular to the plane formed by the two vectors\n",
    "- when you do a cross product, you lose a dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Division\n",
    "\n",
    "just kidding.\n",
    "\n",
    "### Inverses and the Identity Matrix\n",
    "\n",
    "It is **not** possible to divide by matrices (broadcasting still works element-wise). What we can do is find the **inverse** of a matrix. When a matrix is multiplied by its inverse, it results in the identity matrix. \n",
    "\n",
    "<img src = \"./resources/inverse.webp\">\n",
    "\n",
    "The order of multiplication does not matter for a matrix and its inverse:\n",
    "\n",
    "$A \\cdot A^{-1} = A^{-1} \\cdot A $\n",
    "\n",
    "An identity matrix is a square with a diagonal of 1's moving from left to right and the remaining numbers 0. When a matrix is multiplied by an identity matrix, it will result in the same matrix (think of it as the operational equivalent to 1 for linear algebra).\n",
    "\n",
    "<img src = \"./resources/identity_matrix.svg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the effect of multiplying by the identity matrix\n",
    "\n",
    "x = np.array([[4,8,10],[3,9,12],[5,10,15]])\n",
    "i_3 = np.identity(3)\n",
    "\n",
    "print(x, '\\n')\n",
    "print(i_3, '\\n')\n",
    "print(x.dot(i_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of x and multiplying by x\n",
    "\n",
    "inv_x = np.linalg.inv(x)\n",
    "print(inv_x, '\\n')\n",
    "\n",
    "print(np.round(x.dot(inv_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Systems of Linear Equations\n",
    "\n",
    "One of the most common applications of matrix operations is in solving systems of linear equations. \n",
    "***\n",
    "### Sidebar: What are Linear Equations?\n",
    "\n",
    "Linear equations only have **linear variables**. This means our unknowns are only multiplied by a scalar and raised to a power of only **one**, such as:\n",
    "\n",
    "$ x - 2y = 1$\n",
    "\n",
    "$3ex + 2\\pi y = 0$\n",
    "\n",
    "**Not linear:**\n",
    "\n",
    "$ x^2 - 2\\ln{y} = 4$\n",
    "\n",
    "$0.5x + 2y^x = 11$\n",
    "\n",
    "$e^x + 2x=2$\n",
    "\n",
    "***\n",
    "\n",
    "To solve the system of linear equations to find `x` and `y`:\n",
    "\n",
    "`x - 2y = 1`\n",
    "\n",
    "`3x + 2y = 11`\n",
    "\n",
    "First, we need to represent these equations as matrices/vectors.\n",
    "\n",
    "$\\begin{pmatrix}1 & -2 \\\\3 & 2 \\end{pmatrix} \\cdot \\begin{pmatrix}x \\\\ y \\end{pmatrix} = \\begin{pmatrix}1 \\\\11\\end{pmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,-2],[3,2]])\n",
    "b = np.array([[1],[11]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ A \\cdot X = B $\n",
    "\n",
    "$ A^{-1} A X = A^{-1} \\cdot B  $\n",
    "\n",
    "$I \\cdot X   = A^{-1} \\cdot B $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,-2],[3,2]])\n",
    "b = np.array([[1],[11]])\n",
    "inv_a = np.linalg.inv(a)\n",
    "\n",
    "inv_a.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Linear Regression with Linear Algebra (OLS!)\n",
    "\n",
    "A linear regression can be interpreted as the solution to a system of linear equations: each observation just corresponds to a linear equation, and the **coefficients** are the linear unknowns we're solving for! \n",
    "\n",
    "We're representing each **observation** as a **linear combination of features**.\n",
    "\n",
    "Our prediction equation for a linear regression typically looks something like:\n",
    "\n",
    "$ y_{pred} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n $\n",
    "\n",
    "Represented in matrix form:\n",
    "\n",
    "$ y = Xb $, so we are solving for $b$.\n",
    "\n",
    "In this example, we'll work through a linear regression problem with the Auto dataset. We want to predict the **mpg** using *cylinders, displacement, horsepower, weight, acceleration and year*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "car_df = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Auto.csv',na_values='?').dropna()\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = car_df[['cylinders','displacement','horsepower','weight','acceleration','year']]\n",
    "y = car_df['mpg']\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the intercept term\n",
    "X_df['constant'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = Xb + 0 $  --> $ y = Xb $\n",
    "\n",
    "We want to solve for $b$! As we did before, to solve for $b$ we need to multiply both sides by the inverse of $X$.\n",
    "\n",
    "Let's try to $ X^{-1} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(X_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get: \n",
    "\n",
    "    LinAlgError: Last 2 dimensions of the array must be square.\n",
    "\n",
    "We can only calculate an inverse of a **square** matrix. Why?\n",
    "\n",
    "***\n",
    "\n",
    "### Sidebar (again): Some Linear Algebra Theory \n",
    "\n",
    "In Linear Algebra theory, we have something called an **invertible matrix** with a definition as follows:\n",
    "\n",
    "    An n-by-n square matrix A is called invertible if there exists an N by N square matrix B such that\n",
    "\n",
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Georgia; font-size:1.5em;\">AB = BA = I</span></div>\n",
    "\n",
    "    where I is the identity matrix. A and B are inverses of each other.\n",
    "    \n",
    "***\n",
    "    \n",
    "\n",
    "By this definition, we can only find the inverse of square matrices. So with $b$ not being square, how can we solve this system using the data that we have? (No spoilers.)\n",
    "\n",
    "\n",
    " $$b = (X^{T}X)^{-1}X^{T}y$$ \n",
    "\n",
    "\n",
    "\n",
    "Let's apply this to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = (X_df.values).T\n",
    "xtx = np.matmul(xt, X_df.values)\n",
    "\n",
    "product = np.matmul(np.linalg.inv(xtx), xt)\n",
    "\n",
    "b = np.matmul(product, y.values)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our coefficients! They correspond to each of the columns in `X_df` in order. Let's compare this to our `sklearn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_df.columns, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "skl_X = X_df.drop(columns = 'constant')\n",
    "lr.fit(skl_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('constant: ', lr.intercept_)\n",
    "print('coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* 3 Blue 1 Brown:  https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_a\n",
    "* Matrix approach to Linear Regression: http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11\n",
    "* [link to fun desmos interaction](https://www.desmos.com/calculator/yovo2ro9me)\n",
    "* [Link to good video on scalars and vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "* [What is X^T * X?](https://stats.stackexchange.com/questions/267948/intuitive-explanation-of-the-xtx-1-term-in-the-variance-of-least-square/267963)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
